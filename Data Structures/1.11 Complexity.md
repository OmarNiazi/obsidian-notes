# Time Complexity and Big-O Notation: Surgical Review

## 1. Fundamentals and Objectives

### 1.1. Core Definitions

- **Algorithm:** A well-defined computational procedure that takes input and produces output.
    
- **Analysis of Algorithms:** Calculating the resources (time and memory) required by an algorithm as a function of its input size, $n$.
    
- **Time Complexity,** $T(n)$**:** The amount of time (measured in basic operations/steps) an algorithm takes to run to completion.
    
- **Space Complexity:** The amount of memory an algorithm needs (excluding input).
    

### 1.2. The Importance of Asymptotic Analysis

- We use the **Random Access Model (RAM)**, where each basic operation (arithmetic, assignment, comparison, memory access) takes 1 unit of time.
    
- **Goal:** To determine the **growth rate** of $T(n)$ as $n \to \infty$. This rate is independent of hardware, programming language, and constant factors.
    
- **Cases for Analysis:**
    
    - **Worst Case (Upper Bound):** The maximum running time over all inputs of size $n$. Most crucial for analysis (represented by Big-O).
        
    - **Best Case (Lower Bound):** The minimum running time over all inputs of size $n$ (represented by Big-Omega $\Omega$).
        
    - **Average Case (Tight Bound):** The expected running time over all inputs (represented by Big-Theta $\Theta$).
        

### 1.3. Asymptotic Notations

|   |   |   |   |
|---|---|---|---|
|**Notation**|**Reading**|**Definition (Formal)**|**Interpretation**|
|**Big-Oh (**$O$**)**|$f(n)$ is $O(g(n))$|$\exists c, n_0 > 0$ such that $f(n) \le c \cdot g(n)$ for all $n \ge n_0$.|**Upper Bound:** $f(n)$ grows no faster than $g(n)$. Used for **Worst-Case** complexity.|
|**Big-Omega (**$\Omega$**)**|$f(n)$ is $\Omega(g(n))$|$\exists c, n_0 > 0$ such that $f(n) \ge c \cdot g(n)$ for all $n \ge n_0$.|**Lower Bound:** $f(n)$ grows at least as fast as $g(n)$. Used for **Best-Case** complexity.|
|**Big-Theta (**$\Theta$**)**|$f(n)$ is $\Theta(g(n))$|$\exists c_1, c_2, n_0 > 0$ such that $c_1 \cdot g(n) \le f(n) \le c_2 \cdot g(n)$ for all $n \ge n_0$.|**Tight Bound:** $f(n)$ grows at the same rate as $g(n)$. Used for **Average-Case** complexity.|

## 2. Simplification Rules and Memorization

### 2.1. The Three Big-O Simplification Rules (Core Mandate)

These are used to transition from the exact function $T(n)$ to the asymptotic Big-O notation $O(g(n))$.

1. **Ignore Lower-Order Terms:** In an expression $T(n) = a \cdot n^k + b \cdot n^{k-1} + \dots$, the highest-order term dictates the growth rate.
    
    - _Example:_ $T(n) = 3n^2 + 100n + 500 \implies O(n^2)$  
        
2. **Ignore Constant Coefficients of the Highest-Order Term:** The constant factor $c$ in $c \cdot n^k$ is irrelevant for asymptotic growth.
    
    - _Example:_ $T(n) = 3n^2 \implies O(n^2)$  
        
3. **Logarithm Base Ignored:** Changing the base of a logarithm only changes the value by a constant factor, so the base is omitted in Big-O.
    
    - _Rule:_ $\log_a n = \frac{\log_b n}{\log_b a}$. Since $1/\log_b a$ is a constant factor, $O(\log_a n) = O(\log n)$.
        

### 2.2. Essential Formulas to Memorize (From Rule 0 & 00)

|   |   |   |
|---|---|---|
|**Formula**|**Description**|**Simplification â†’O()**|
|$\sum_{i=1}^{n}i = \frac{n(n+1)}{2} = \frac{n^2+n}{2}$|Sum of the first $n$ integers|$O(n^2)$|
|$\sum_{i=1}^{n}i^2 = \frac{n(n+1)(2n+1)}{6}$|Sum of the first $n$ squares|$O(n^3)$|
|$\sum_{k=0}^{m}r^{k} = \frac{1-r^{m+1}}{1-r}$|Geometric Series (for recursive divisions)|$O(r^m)$ (if $r>1$) or $O(1)$ (if $0<r<1$)|
|$\log_x z = y$ if $x^y = z$|Logarithm definition|(Used in logarithmic loop analysis)|

## 3. Rules for Loop Analysis (From Complexity Rules)

These rules are your checklist for determining $T(n)$ for iterative structures.

|   |   |   |   |
|---|---|---|---|
|**Rule**|**Code Pattern**|**Iteration Count T(n)**|**Big-O Result**|
|**Linear (Fixed Step)**|`for (i=0; i < n; i = i + k)`|$\approx n/k$|$O(n)$|
|**Linear (Decrement)**|`for (i=n; i > 0; i = i - k)`|$\approx n/k$|$O(n)$|
|**Logarithmic (Multiplication)**|`for (i=1; i < n; i = i * k)`|$\approx \log_k n$|$O(\log n)$|

### 3.1. Rules for Nested Loops

|   |   |   |   |
|---|---|---|---|
|**Case**|**Code Pattern**|**Iteration Count T(n)**|**Big-O Result**|
|**Independent Variables (Rule 6)**|`for (i=0; i<n; i++) { for (j=0; j<m; j++) ... }`|$n \cdot m$|$O(n \cdot m)$ or $O(n^2)$ if $n=m$|
|**Dependent Variables (Triangular Sum - Rule 4)**|`for (i=1; i<=n; i++) { for (j=1; j<=i; j++) ... }`|$\sum_{i=1}^{n} i = \frac{n(n+1)}{2}$|$O(n^2)$|
|**Logarithmic Outer / Linear Inner (Rule 5)**|`for (i=1; i<=n; i=i*2) { for (j=1; j<=i; j++) ... }`|$\sum_{k=0}^{\log_2 n} 2^k \approx 2n-1$|$O(n)$|

### 3.2. Sequential and Conditional Statements

- **Sequence of Statements:** If operations are sequential, add their complexities. The total complexity is dominated by the highest order term.
    
    - $O(n^2) + O(n) + O(1) = O(n^2)$  
        
- **Conditional Statements (IF/ELSE):** The running time is the time of the condition check plus the maximum running time of the two branches.
    
    - $T(\text{if/else}) = O(\text{Condition}) + \max(O(\text{Statement 1}), O(\text{Statement 2}))$.
        

## 3.3. Deep Dive: Logarithmic Complexity $O(\log^k n)$  

Logarithmic complexity is characterized by a **geometric progression** in the loop variable or the problem size (in recursion). The key is to determine the exponent $x$ such that the loop terminates, i.e., $k^x \approx n$.

#### Pattern I: Simple Logarithm $O(\log n)$  

|   |   |   |
|---|---|---|
|**Code Pattern**|**Iteration Analysis**|**Calculation and Simplification**|
|`for (i=1; i < n; i = i * k)`|The values of $i$ are: $k^0, k^1, k^2, \dots, k^x$. The loop stops when $k^x \ge n$. Using $\log_k$ on both sides: $x \ge \log_k n$.|$\text{Iterations} = \lceil \log_k n \rceil + 1 \approx O(\log_k n)$ $\mathbf{Big\text{-}O} = O(\log n)$ (Since base $k$ is a constant, it is ignored).|
|**Ex. 6:** `for (i=1; i<n; i=i*2) sum++;`|Here $k=2$. The number of steps for `sum++` is $\approx \log_2 n$.|$\mathbf{T(n)} \approx 3 \log_2 n + 3$ (init/test/body/final). $\mathbf{Big\text{-}O} = O(\log n)$|

#### Pattern II: Linear Outer / Logarithmic Inner $O(n \log n)$  

|   |   |   |
|---|---|---|
|**Code Pattern**|**Iteration Analysis**|**Calculation and Simplification**|
|`for (i=0; i<n; i++) { for (j=1; j<n; j=j*k) sum++; }`|**Outer Loop (i):** Runs $n$ times. **Inner Loop (j):** Runs $\log_k n$ times.|$\mathbf{T(n)} = \sum_{i=1}^{n} (\text{cost of inner loop}) = \sum_{i=1}^{n} O(\log_k n) = n \cdot O(\log_k n)$ $\mathbf{Big\text{-}O} = O(n \log n)$|
|**Ex. 7:** `for (i=1; i<n; i=i*2) { for (j=0; j<n; j++) sum++; }`|**Outer Loop (i):** Runs $\log_2 n$ times. **Inner Loop (j):** Runs $n$ times.|$\mathbf{T(n)} = \sum_{k=1}^{\log_2 n} n = n \cdot \log_2 n$ $\mathbf{Big\text{-}O} = O(n \log n)$|

#### Pattern III: Nested Logarithmic $O(\log^2 n)$  

|   |   |   |
|---|---|---|
|**Code Pattern**|**Iteration Analysis**|**Calculation and Simplification**|
|`for (i=1; i<n; i=i*k) { for (j=1; j<i; j=j*m) sum++; }`|**Outer Loop (i):** Runs $\log_k n$ times. **Inner Loop (j):** Runs $\log_m i$ times.|**Total Cost** $T(n) = \sum_{i=k^0}^{k^{\log_k n}} \log_m i$. This sum is complex, but the _maximum_ value is $O(\log_k n \cdot \log_m n)$. **Big-O (Upper Bound)** is $O(\log^2 n)$.|
|**Ex. 12:** `for (i=1; i<=n; i=i*2) { for (j=1; j<=i; j=j*2) cout<<"*"; }`|**Outer Loop (i):** $i$ takes values $2^0, 2^1, \dots, 2^{\log_2 n}$ ($\log_2 n + 1$ iterations). **Inner Loop (j):** Runs $\log_2 i$ times.|$T(n) = \sum_{k=0}^{\log_2 n} \log_2 (2^k) = \sum_{k=0}^{\log_2 n} k$. This is the sum of integers $1$ to $L$ where $L=\log_2 n$. $T(n) \approx \frac{L(L+1)}{2}$. **Big-O** is $O(L^2) \implies O(\log^2 n)$.|

#### Pattern IV: Double Logarithm $O(\log(\log n))$  

|   |   |   |
|---|---|---|
|**Code Pattern**|**Iteration Analysis**|**Calculation and Simplification**|
|`for (i=2; i<n; i = i*i) { sum++; }`|The values of $i$ are: $2, 2^2, (2^2)^2=2^4, (2^4)^2=2^8, \dots, 2^{2^x}$. The loop stops when $2^{2^x} \ge n$.|1. Take $\log_2$ of both sides: $\log_2 (2^{2^x}) \ge \log_2 n \implies 2^x \ge \log_2 n$. 2. Take $\log_2$ again: $\log_2 (2^x) \ge \log_2 (\log_2 n) \implies x \ge \log_2 (\log_2 n)$. $\mathbf{Big\text{-}O} = O(\log(\log n))$.|

## 4. Edge Cases and Traps to Avoid

### 4.1. Edge Case 1: Constant Loop Bounds

If a loop's bounds are fixed numbers, it is $O(1)$, regardless of how large the input $n$ is outside the loop.

- **Trap Example:**
    
    ```
    for (int i = 0; i < 1000; i++) {
        // ... O(1) operations
    }
    
    ```
    
- **Complexity:** $O(1000) \implies O(1)$. The number of operations is independent of $n$.
    

### 4.2. Edge Case 2: Recursion with Non-Linear Division (Question 1.i)

When a recursive function divides $n$ by a constant $k > 1$.

- **Code Pattern:**
    
    ```
    void func(int n) { 
        if (n > 0) { 
            func(n/k); 
        } 
    }
    
    ```
    
- **Complexity:** $O(\log_k n) \implies O(\log n)$. The input size reduces geometrically, similar to Binary Search.
    

### 4.3. Edge Case 3: Space Complexity Trap (Question 1.ii)

In time complexity, $T(n)$, we typically ignore memory allocation. But if memory allocation depends on $n$, it affects the total run time and space complexity.

- **Code Pattern (Focus on Time):**
    
    ```
    for (int i = 0; i < n; i++) {
        for (int j = 0; j <= 999; j++) { // This loop runs 1000 times (Constant)
            // ... O(1) operations
            int* array = new int[i*j]; // O(1) time operation (only allocation request)
        }
    }
    
    ```
    
- **Time Complexity:** The inner loop is $O(1000) \implies O(1)$. The outer loop is $O(n)$. Total time complexity is $O(n) \cdot O(1) = O(n)$.
    
- **Space Complexity (The Trap):** Since memory is allocated in the loop and depends on $n$, the total space used over the lifetime of the function can be calculated by summing the maximum amount of space allocated.
    
    - The `new int[num]` allocates space. If the array is destroyed at the end of the inner loop, the **Auxiliary Space** is the largest array created, $\approx O(n^2)$. If it persists, the total space is much higher. For a typical analysis of _auxiliary space_, we look at the peak memory used.
        

### 4.4. Edge Case 4: Base Change in Exponentials (Question 2 & 3)

- **Rule:** $c^{n+k} = c^k \cdot c^n$. Since $c^k$ is a constant, $O(c^{n+k}) = O(c^n)$. (Question 2: $2^{n+1} = 2 \cdot 2^n \implies O(2^n)$).
    
- **Trap:** $(c^k)^n = c^{kn}$. This is **NOT** the same complexity. The base matters in the exponent. (Question 3: $2^{2n} = (2^2)^n = 4^n$. $O(4^n)$ is NOT $O(2^n)$).
    
    - $4^n$ grows exponentially faster than $2^n$.
        

## 5. Worked Examples (From Exercises)

|   |   |   |
|---|---|---|
|**Exercise Code Pattern**|**Time Calculation T(n)**|**Big-O Result O(n)**|
|**Ex. 1:** `for (i=0; i<n; i++) sum++;`|$1 (\text{init}) + (n+1) (\text{test}) + n (\text{incr}) + n (\text{body}) + 1 (\text{final}) = 3n+3$|$O(n)$|
|**Ex. 2:** `for (i=0; i<n; i++) { for (j=0; j<n; j++) sum++; }`|$\approx n^2 (\text{inner}) + \dots = 3n^2+4n+3$|$O(n^2)$|
|**Ex. 3:** `for (i=0; i<n; i=i+2) sum++;`|$n/2$ iterations $\approx 3(n/2)+3$|$O(n)$|
|**Ex. 7:** `for (i=1; i<n; i=i*2) { for (j=0; j<n; j++) sum++; }`|Outer loop: $\log_2 n$ iterations. Inner loop: $n$ iterations. Total: $n \cdot \log_2 n$|$O(n \log n)$|
|**Ex. 8 (Triangular Sum):** `for (i=1; i<=n; i++) { for (j=1; j<=i; j++) sum++; }`|$\sum_{i=1}^{n} i = O(n^2)$|$O(n^2)$|

### 5.1. Advanced Nested Logarithmic Example (Question 12)

**Code:**

```
for (int i = 1; i <= n; i = i * 2) { 
    for (int j = 1; j <= i; j = j * 2) { 
        cout << "*"; 
    } 
}

```

**Analysis:**

1. **Outer Loop (`i`):** Runs $\log_2 n$ times. (Values: $1, 2, 4, 8, \dots, n$).
    
2. **Inner Loop (`j`):** The number of times the inner loop runs depends on the current value of $i$.
    
    - For a fixed $i$, the inner loop runs $\log_2 i$ times.
        
3. **Total Cost** $T(n)$**:** Sum of the inner loop counts for all outer loop iterations:
    
    $$T(n) = \sum_{k=0}^{\log_2 n} (\text{Inner loop count when } i=2^k)$$$$T(n) = \sum_{k=0}^{\log_2 n} \log_2 (2^k) = \sum_{k=0}^{\log_2 n} k$$
4. **Result:** This is the sum of integers from $0$ up to $\log_2 n$. Let $L = \log_2 n$.
    
    $$T(n) = \sum_{k=1}^{L} k = \frac{L(L+1)}{2} = \frac{(\log_2 n)(\log_2 n + 1)}{2}$$
5. **Big-O:** Ignoring lower terms and constants:
    
    $$T(n) \approx \frac{1}{2} (\log_2^2 n + \log_2 n) \implies O(\log^2 n)$$